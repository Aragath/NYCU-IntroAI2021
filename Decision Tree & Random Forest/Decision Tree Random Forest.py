# -*- coding: utf-8 -*-
"""0816039_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KuQMmH9xdJlXHYtZ8ZxjDIaThPbMtQWr

# Assignment 2: Decision Trees

In this assignment, you are going to implement a decision tree (or random forest) to forcast the weather.

## Description

- You must implement a `Model` class for training and predicting:
  ```python
  X, y = load_dataset()

  model = Model(num_features, num_classes)

  # training
  model.fit(X, y)

  # prediction
  y_pred = model.predict(X)
  ```
- Please search (Ctrl+F) for `TODO` to see what you need to do.
- About the dataset
  - Given the **training set**, please **train/validate** on it.  
  (note that your model will get bad testing score if it overfits on the training set)
  - After submitting the assignment, we will train on the same training set and test on the hidden **testing set** for scoring (using [f1-score](https://towardsdatascience.com/a-look-at-precision-recall-and-f1-score-36b5fd0dd3ec#11b8)).

### Typical performance

- **Random Guess**  
  F1-Score: 0.30  
  Accuracy: 0.50
- **Always Predict 1**  
  F1-Score: 0.37  
  Accuracy: 0.22
- **Always Predict 0**  
  F1-Score: 0.00  
  Accuracy: 0.77
- **sklearn.tree.DecisionTreeClassifier**  
  - **Training (5-fold cross-validation mean)**  
    F1-Score: 0.63-0.99  
    Accuracy: 0.85-0.99
  - **Validation (5-fold cross-validation mean)**  
    F1-Score: 0.50-0.60  
    Accuracy: 0.75-0.90
"""

###########################
# DO NOT CHANGE THIS CELL #
###########################

import os
import urllib.request
import numpy as np
import pandas as pd
from typing import Tuple
from sklearn.model_selection import KFold
from sklearn.metrics import f1_score, accuracy_score


def load_dataset(url):
  """ Get and load weather dataset. """

  path = url.split('/')[-1] # get the file name from url

  if not os.path.exists(path):
    print('Download:', url)
    urllib.request.urlretrieve(url, path)

  return pd.read_pickle(path) # pickle protocol=4


def get_input_target(df):
  """ Get X and y from weather dataset. """
  
  target_column = 'RainTomorrow' # predict 1 if it rains tomorrow

  X = df.drop(columns=[target_column]).to_numpy() # drop means to abandon target column, to_numpy means to change to an array, xæ˜¯é™¤äº†RainTomorrowçš„å¤šç¶­é™£åˆ—
  y = df[target_column].to_numpy() # to_numpy means to change to an array, yæ˜¯æ©«çš„ä¸€ç¶­é™£åˆ— [0, 0, 0, 0, 0]

  return X, y


def k_fold_cv(model_create_fn, X, y, k=5):
  """ Run k-fold cross-validation. """

  results = []

  idxs = list(range(X.shape[0])) # .shape[0] å›žå‚³ç¬¬0åˆ—çš„å¤§å° ref: https://www.itread01.com/content/1540479380.html
                   # ref: https://blog.csdn.net/qq_28618765/article/details/78081959
                   # list(range(10)) = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
  np.random.shuffle(idxs)

  for i, (train_idxs, val_idxs) in enumerate(KFold(k).split(idxs)):
    splits = {'train': (X[train_idxs], y[train_idxs]),
              'val':   (X[val_idxs],   y[val_idxs]  )}

    print('Run {}:'.format(i+1))

    model = model_create_fn()
    model.fit(*splits['train']) # training

    for name, (X_split, y_split) in splits.items():
      y_pred = model.predict(X_split)

      result = {'split': name,
                'f1': f1_score(y_pred, y_split),
                'acc': accuracy_score(y_pred, y_split)}
      results.append(result)

      print('{split:>8s}: f1={f1:.4f} acc={acc:.4f}'.format(**result))

  return pd.DataFrame(results)


# @begin

# TODO: you can define or import something here (optional)

class Model: # = DecisionTreeClassifier()

  def __init__(self, num_features: int, num_classes: int):
    """
    Initialize the model.

    Args:
        num_features (int) : the input feature size.
        num_classes (int) : number of output classes.
    """

    self.num_features = num_features
    self.num_classes = num_classes

    # TODO: implement your model initialization here (optional)
    self.tree = None

  def fit(self, X: np.ndarray, y: np.ndarray):
    """
    Train on input/target pairs.

    Args:
        X (np.ndarray) : training inputs with shape (num_inputs, num_features).
        y (np.ndarray) : training targets with shape (num_inputs,).
    """

    # TODO: implement your training algorithm here
    num_inputs = X.shape[0]
    y.reshape(num_inputs, 1)
    train = np.concatenate((X,y[:,None]), axis=1) # combining X and y into one train array. ref: https://stackoverflow.com/questions/30305069/numpy-concatenate-2d-arrays-with-1d-array
    self.tree = build_tree(train, max_depth = 2, min_size = num_inputs/(2**2)) # modify max_depth and min_size for different decision tree


  def predict(self, X: np.ndarray) -> np.ndarray:
    '''
    Predict y given X.

    Args:
        X (np.ndarray) : inputs, shape: (num_inputs, num_features).
    
    Returns:
        np.ndarray : the predicted integer outputs, shape: (num_inputs,).
    '''

    # TODO: implement your prediction algorithm here
    predictions = list()
    for row in X:
      prediction = predict_(self.tree, row)
      predictions.append(prediction)
    return(predictions)

# ref: https://www.youtube.com/watch?v=-W0DnxQK1Eo > concept for decision tree
# ref: https://www.youtube.com/watch?v=tfGuRVTwxq8 > python code for using sklearn to implment decision tree
# ref: https://gist.github.com/djosix/e1e86fc91fe60cff86ce0626b9aa020b > TA's example code to fasten the split function
# ref: https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/ > how to implement code from scratch

# Calculate the Gini index for a split dataset
def gini_index(groups, classes):
  '''
  input ex:
  groups = [[[1, 1], [1, 0]], [[1, 1], [1, 0]]]
  classes = [0, 1] 

  '''
	# count all samples at split point
  num_instances = float(sum([len(group) for group in groups]))
	# sum weighted Gini index for each group
  gini = 0.0
  for group in groups:
    size = float(len(group))
    # avoid divide by zero
    if size == 0:
      continue
    score = 0.0
    # score the group based on the score for each class
    for class_val in classes:
      p = [row[-1] for row in group].count(class_val) / size # list.count(X) means to count how many times X appears in the list
                                        # ref: https://www.guru99.com/python-list-count.html
      score += p * p
    # weight the group score by its relative size
    gini += (1.0 - score) * (size / num_instances)
  return gini

# Split a dataset based on an attribute and an attribute value
def test_split(index, value, dataset):
  low_idxs = dataset[:, index] <= value
  left = dataset[low_idxs]
  right = dataset[~low_idxs]
  return left, right
  
# Select the best split point for a dataset
def get_split(dataset):
  #class_values = list(set(row[-1] for row in dataset)) # set() ref: https://www.runoob.com/python/python-func-set.html
  class_values = list([0, 1])
  b_index, b_value, b_score, b_groups = 999, 999, 999, None
  dataset = np.array(dataset) # ref: https://stackoverflow.com/questions/15884527/how-can-i-prevent-the-typeerror-list-indices-must-be-integers-not-tuple-when-c
  for index in range(len(dataset[0])-1):
    ds = dataset[:, index] # å–ç¬¬indexå€‹featureçš„å€¼
    ds = np.unique(ds) # å°‡arrayè½‰ç‚º1Dä¸¦åˆªé™¤é‡è¤‡å€¼
    ds = (ds[1:] + ds[:-1]) / 2

    row_num = len(dataset)
    dataset_ = dataset[:int(0.35*row_num), :] # only calculate with the first 0.35 of the dataset, too long to test all datas

    for val in ds:
      groups = test_split(index, val, dataset_)
      gini = gini_index(groups, class_values)
      if gini < b_score:
        b_index, b_value, b_score, b_groups = index, val, gini, groups
  b_groups = [group.tolist() for group in b_groups]
  return {'index':b_index, 'value':b_value, 'groups':b_groups}

# Create a terminal node value
def to_terminal(group):
  outcomes = [row[-1] for row in group]
  return max(set(outcomes), key=outcomes.count) # return the most frequent class
   
# Create child splits for a node or make terminal
def split(node, max_depth, min_size, depth):
	#print("now in split")
	left, right = node['groups']
	del(node['groups'])
	# check for a no split
	if not left or not right:
		node['left'] = node['right'] = to_terminal(left + right)
		return
	# check for max depth
	if depth >= max_depth:
		node['left'], node['right'] = to_terminal(left), to_terminal(right)
		return
	# process left child
	if len(left) <= min_size:
		node['left'] = to_terminal(left)
	else:
		node['left'] = get_split(left)
		split(node['left'], max_depth, min_size, depth+1)
	# process right child
	if len(right) <= min_size:
		node['right'] = to_terminal(right)
	else:
		node['right'] = get_split(right)
		split(node['right'], max_depth, min_size, depth+1)
    
# Build a decision tree
def build_tree(train, max_depth, min_size):
  root = get_split(train)
  split(root, max_depth, min_size, 1)
  return root
    
# Make a prediction with a decision tree
def predict_(node, row):
	if row[node['index']] < node['value']: # check if its value is smaller than the node value
		if isinstance(node['left'], dict): # check whether left node is dict
			return predict_(node['left'], row)
		else:
			return node['left']
	else:
		if isinstance(node['right'], dict):
			return predict_(node['right'], row)
		else:
			return node['right']

# @end

###########################
# DO NOT CHANGE THIS CELL #
###########################

df = load_dataset('https://lab.djosix.com/weather.pkl')
X_train, y_train = get_input_target(df)

df.head() # head() means to print out

###########################
# DO NOT CHANGE THIS CELL #
###########################

create_model = lambda: Model(X_train.shape[1], 2) # same as clf = DecisionTreeClassifier(), create_model = clf & Model = DecisionTreeClassifier
k_fold_cv(create_model, X_train, y_train).groupby('split').mean()

"""## Submission

1. Make sure your code runs correctly after clicking `"Runtime" > "Restart and run all"`
2. Rename this notebook to `XXXXXXX_2.ipynb`, where `XXXXXXX` is your student ID.
3. Download IPython notebook: `"File" > "Download" > "Download .ipynb"`
4. Download Python source code: `"File" > "Download" > "Download .py"`
5. Create a zip file for `XXXXXXX_2.ipynb` and `XXXXXXX_2.py`  
   named `XXXXXXX_2.zip`, where `XXXXXXX` is your student ID.
6. Upload the zip file to E3.

ðŸ˜Š Good luck!
"""